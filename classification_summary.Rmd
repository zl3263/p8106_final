---
title: "p8106_final_classification"
author: "Zekai Jin, Zizhao Lin, Jiawen Zhao"
date: "2023-5-8"
output: html_document
---


```{r, echo=FALSE}
knitr::opts_chunk$set(
  message  = FALSE,
  warning = FALSE
)
```


```{r}
# Load the packages
library(tidyverse)
library(forcats)
library(ggridges)
library(corrplot)
library(patchwork)
library(caret)
library(klaR)
library(kernlab)
library(factoextra)
library(ISLR)
library(mlbench)
library(caret)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(pROC)
library(randomForest)
library(ranger)
library(gbm)
library(doParallel)
library(vip)

# prepare the dataset
load("data/recovery.RData")

# sample from data
set.seed(2357) 
sample_1=sample(1:10000, 2000)
set.seed(3263) 
sample_2=sample(1:10000, 2000)

sample = c(sample_1,sample_2) %>% unique()

set.seed(1)

# preprocessing
dat = 
  dat[sample,] %>%
  janitor::clean_names() %>%
  mutate(
    gender=fct_recode(factor(gender),male='1',female='0'),
    race=fct_recode(factor(race),white='1',asian='2',black='3',hispanic='4'),
    smoking=fct_recode(factor(smoking),never='0',former='1',current='2'),
    hypertension=factor(hypertension),
    diabetes=factor(diabetes),
    vaccine=factor(vaccine),
    severity=factor(severity),
    study=factor(study),
    recovery_bin = if_else(recovery_time <= 30, 'f', 's'), 
    recovery_bin = factor(recovery_bin),
  )%>%
  dplyr::select(-id, -recovery_time)


# train-test splitting
sample = sample(c(TRUE, FALSE), nrow(dat), replace=TRUE, prob=c(0.8,0.2))
data_train = dat[sample,]
data_test = dat[!sample,]

# for parameter tuning
ctrl <- trainControl(method = "cv", 
                     #summaryFunction = twoClassSummary,
                     classProbs = TRUE)

```

<!--
## PCA (不要了)

According to PCA, the two classes are non-separable. 

 分类结果都好烂，用pca看看好不好分 

```{r}
pca = prcomp(data.matrix(data_train))

fviz_eig(pca, addlabels = TRUE)

fviz_pca_ind(pca,
             habillage = data_train$recovery_bin,
             label = "none",
             addEllipses = TRUE)
```

-->

## Classification

## GLM 

General liner model is tested  with logit link for binary outcome. It has no tuning parameter available and the accuracy is 0.71 in cross validation.

```{r}
set.seed(1)
glm_fit = train(recovery_bin ~ . , 
                data_train,
                method = "glm",
                trControl=ctrl,
                family = binomial(link = "logit"))


summary(glm_fit)
```

## LDA 

Discriminate Analysis assign a sample to the group which it presents higher probability under given predictor. According to the plot, little difference is shown in two group, thus it fail to classify the data.  

<!-- qda表现也很烂就不赘述了 -->

```{r}

lda_fit0 = lda(recovery_bin~., data = data_train)
plot(lda_fit0)

# Accordingt to plot, LDA failed to separate the two classes. 

# refit using caret for model comparation



set.seed(1)
lda_fit = train(x = data.matrix(data_train[1:14]),
                y = data_train$recovery_bin,
                method = "lda",
                trControl = ctrl)

lda_pred_raw = predict(lda_fit, newdata = data.matrix(data_test), type = "raw")
confusionMatrix(data = lda_pred_raw, reference = data_test$recovery_bin)

```


## SVM -- sth wrong?

Support Vector Machine separates data at hyper plane. Linear SVM presents little difference on cost ranging from -2 to 3. SVM with radial sigma is optimized when cost is around 20 and sigma is around -8.3, which reaches an accuracy of 0.71. The SVM has similar result comparing to GLM, which is reasonable since the data is non-seperatable. 


<!-- 跑不动， 全寄，看泽楷的图说话.jpg -->
## linear kernel
```{r cache=TRUE}
# parallel computation
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

set.seed(1)
svml_fit = train(recovery_bin~.,
                 data_train,
                 method = "svmLinear",
                 tuneGrid = data.frame(C = exp(seq(-2,3,len=30))),
                 preProcess = c("center","scale"),
                 trControl = ctrl)

stopCluster(cl)

plot(svml_fit, highlight = TRUE, xTrans = log)
```

## radial kernel

```{r cache=TRUE}
svmr.grid = expand.grid(C = exp(seq(-1,5,len=20)),
                        sigma = exp(seq(-10,-1,len=20)))

# parallel computation
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

set.seed(1)
svmr.fit = train(x = data.matrix(data_train[1:14]),
                 y = data_train$recovery_bin,
                 method = "svmRadialSigma",
                 tuneGrid = svmr.grid,
                 trControl = ctrl)

stopCluster(cl)

myCol= rainbow(25)

myPar = list(superpose.symbol = list(col = myCol),superpose.line = list(col = myCol))

plot(svmr.fit, highlight = TRUE, par.settings = myPar,xTrans=log)

```

## quadratic kernel

```{r cache=TRUE}
svmq.grid = expand.grid(
  degree=c(2),
  C = exp(seq(-1,5,len=10)),
  scale = c(1))

# parallel computation
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

set.seed(1)
svmq.fit = train(x = data.matrix(data_train[1:14]),
                 y = data_train$recovery_bin,
                 method = "svmPoly",
                 tuneGrid = svmq.grid,
                 trControl = ctrl)

stopCluster(cl)

plot(svmq.fit, highlight = TRUE, par.settings = myPar,xTrans=log)

```


## tree-based models

## single tree

```{r}
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)

set.seed(1)

rpart.fit <- train(recovery_bin ~ . ,
                   data = data_train,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-8,-3, len = 40))),
                   trControl = ctrl)

ggplot(rpart.fit, highlight = TRUE)

```

## adaboost
```{r, cache=TRUE}
tune_grid <- expand.grid(
  n.trees = c(500,1000,2000,3000,4000,5000),
  interaction.depth = 1:7,
  shrinkage = c(0.005,0.001,0.01),
  n.minobsinnode = c(1)
)

# parallel computation
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

set.seed(1)

model_gbm = train(
  recovery_bin~.,
  data=data_train,
  method = "gbm",
  distribution = "adaboost",
  trControl = ctrl,
  tuneGrid=tune_grid,
  verbose = FALSE
)

stopCluster(cl)

ggplot(model_gbm,highlight=TRUE)
```




# comparing different models:
```{r}
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)

comparasion = resamples(list(
  glm=glm_fit,
  lda=lda_fit,
  svm_linear=svml_fit,
  svm_radial=svmr.fit,
  svm_quadratic=svmq.fit,
  tree= rpart.fit,
  tree_boost = model_gbm
  ))

model_rank =
  comparasion$values %>%
  dplyr::select(ends_with("Accuracy")) %>%
  pivot_longer('glm~Accuracy':'tree_boost~Accuracy',names_to = "model", values_to = "Accuracy",names_pattern = "(.*)~Accuracy") %>%
  group_by(model) %>%
  summarize(Accuracy_mean=mean(Accuracy),RMSE_sd = sd(Accuracy)) %>%
  arrange(Accuracy_mean) 
model_rank

model_rank=pull(model_rank,model)

comparasion$values %>%
  dplyr::select(ends_with("Accuracy")) %>%
  pivot_longer('glm~Accuracy':'tree_boost~Accuracy',names_to = "model", values_to = "Accuracy",names_pattern = "(.*)~Accuracy") %>%
  mutate(model=factor(model,levels=model_rank)) %>%
  ggplot() +
  geom_density_ridges(aes(x=Accuracy,y=model,color=model,fill=model),alpha=0.5)
```

# final model evaluation
```{r}
# test error
predicted = predict(model_gbm,newdata=data_test)
mean(data_test$recovery_bin==predicted)


# vip
vip(
  model_gbm,
  method = "permute",
  train = data_train,
  target = "recovery_bin",
  metric = "Accuracy",
  nsim = 10,
  pred_wrapper = predict,
  geom = "boxplot",
  all_permutations = TRUE,
  mapping = aes_string(fill = "Variable")
)

```










