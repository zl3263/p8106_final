---
title: "p8106_final"
author: "Zekai Jin, Zizhao Lin, Jiawen Zhao"
date: "2023-4-30"
output: html_document
---


```{r, echo=FALSE}
knitr::opts_chunk$set(
  message  = FALSE,
  warning = FALSE
)
```


```{r}
# Load the packages
library(tidyverse)
library(forcats)
library(ggridges)
library(corrplot)
library(patchwork)
library(caret)
library(klaR)
library(kernlab)
library(factoextra)

# prepare the dataset
load("data/recovery.RData")

# sample from data
set.seed(2357) 
sample_1=sample(1:10000, 2000)
set.seed(3263) 
sample_2=sample(1:10000, 2000)

sample = c(sample_1,sample_2) %>% unique()

set.seed(1)

# preprocessing
dat = 
  dat[sample,] %>%
  janitor::clean_names() %>%
  mutate(
    gender=fct_recode(factor(gender),male='1',female='0'),
    race=fct_recode(factor(race),white='1',asian='2',black='3',hispanic='4'),
    smoking=fct_recode(factor(smoking),never='0',former='1',current='2'),
    hypertension=factor(hypertension),
    diabetes=factor(diabetes),
    vaccine=factor(vaccine),
    severity=factor(severity),
    study=factor(study),
    recovery_bin = if_else(recovery_time <= 30, 'f', 's'), 
    recovery_bin = factor(recovery_bin),
  )%>%
  dplyr::select(-id, -recovery_time)


# train-test splitting
sample = sample(c(TRUE, FALSE), nrow(dat), replace=TRUE, prob=c(0.8,0.2))
data_train = dat[sample,]
data_test = dat[!sample,]

# for parameter tuning
ctrl <- trainControl(method = "cv", number = 5)

```

## PCA

According to PCA, the two classes are non-separable. 

<!-- 分类结果都好烂，用pca看看好不好分 -->
<!-- 寄-->
```{r}
pca = prcomp(data.matrix(data_train))

fviz_eig(pca, addlabels = TRUE)

fviz_pca_ind(pca,
             habillage = data_train$recovery_bin,
             label = "none",
             addEllipses = TRUE)
```



## Classification

## GLM 

General liner model is tested  with logit link for binary outcome. It has no tuning parameter available and the accuracy is 0.71 in cross validation.

```{r}

#ctrl = trainControl(method = "cv", 
#                     summaryFunction = twoClassSummary,
#                     classProbs = TRUE)

glm_fit = train(recovery_bin ~ . , 
                data_train,
                method = "glm",
                family = binomial(link = "logit"))


summary(glm_fit)

glm_pred_raw = predict(glm_fit, newdata = data_test, type = "raw")

confusionMatrix(data = glm_pred_raw, reference = data_test$recovery_bin)
```

## LDA 

Discriminate Analysis assign a sample to the group which it presents higher probability under given predictor. According to the plot, little difference is shown in two group, thus it fail to classify the data.  

<!-- qda表现也很烂就不赘述了 -->

```{r}

ctrl = trainControl(method = "cv", number = 5, classProbs = TRUE)

lda_fit0 = lda(recovery_bin~., data = data_train)
plot(lda_fit0)

# Accordingt to plot, LDA failed to separate the two classes. 

# refit using caret for model comparation



set.seed(1)
lda_fit = train(x = data.matrix(data_train[1:14]),
                y = data_train$recovery_bin,
                method = "lda",
                metric = "ROC",
                trControl = ctrl)

lda_pred_raw = predict(lda_fit, newdata = data.matrix(data_test), type = "raw")
confusionMatrix(data = lda_pred_raw, reference = data_test$recovery_bin)

```


## SVM -- sth wrong?

Support Vector Machine separates data at hyper plane. Linear SVM presents little difference on cost ranging from -2 to 3. SVM with radial sigma is optimized when cost is around 20 and sigma is around -8.3, which reaches an accuracy of 0.71. The SVM has similar result comparing to GLM, which is reasonable since the data is non-seperatable. 


<!-- 跑不动， 全寄，看泽楷的图说话.jpg
```{r message=FALSE}
ctrl = trainControl(method = "cv", classProbs = TRUE)
set.seed(1)
svml_fit = train(recovery_bin~.,
                 data_train,
                 method = "svmLinear",
                 tuneGrid = data.frame(C = exp(seq(-2,3,len=30))),
                 preProcess = c("center","scale"),
                 trControl = ctrl)

plot(svml_fit, highlight = TRUE, xTrans = log)
```

```{r}

ctrl = trainControl(method = "cv", classProbs = TRUE)
svmr.grid = expand.grid(C = exp(seq(-1,5,len=10)),
                        sigma = exp(seq(-10,-2,len=10)))
set.seed(1)
svmr.fit = train(x = data.matrix(data_train[1:14]),
                 y = data_train$recovery_bin,
                 method = "svmRadialSigma",
                 tuneGrid = svmr.grid,
                 trControl = ctrl)

myCol= rainbow(25)

myPar = list(superpose.symbol = list(col = myCol),superpose.line = list(col = myCol))

plot(svmr.fit, highlight = TRUE, par.settings = myPar)

```

-->

```{r}

```